<!DOCTYPE html>
<html lang="en" class="scroll-smooth">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="fav.svg" type="image/x-icon">
  <title>AI Safety Governance - FrameworkZero.org</title>
  <meta property="og:image" content="logo.svg" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="300" />
  <meta property="og:image:alt" content="Hybrid-Technical AI Safety Governance - FrameworkZero.org" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Sansation:wght@700&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Flex:opsz,wght@400;800&display=swap" rel="stylesheet">
  <link href="style.min.css" rel="stylesheet">
  <script src="scripts.js" defer async></script>
</head>

<body class="text-gray-200 subpixel-antialiased">
  <div class="background-grad fixed top-0 bottom-0 right-0 left-0"></div>

  <div id="tp-container" class="invisible fixed bottom-0 left-0 right-0 py-6 px-6 md:px-24 z-50
      text-base text-center text-white border-t-2 secondary-b
      max-h-36 overflow-y-auto shadow-lg">
  </div>
  <hr class="fixed bottom-0 left-0 right-0 hr-gradient h-2 z-40">

  <div id="main" class="invisible">

    <!-- Context Menu -->
    <nav class="context-menu">
      <div class="menu-button" id="menu-button">
        <!-- New menu icon as requested -->
        <svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
          stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 2L2 7l10 5 10-5-10-5z"></path>
          <path d="M2 17l10 5 10-5M2 12l10 5 10-5"></path>
        </svg>
      </div>
      <div class="menu-list-container" id="menu-list">
        <!-- Doc Version (menu) -->
        <div class="mt-1 ml-4 select-none text-red-400">Version <span class="version-number"></span></div>
        <!-- Menu items will be dynamically populated -->
        <ul></ul>
      </div>
    </nav>

    <div class="container max-w-6xl mx-auto px-4 py-12 md:px-6">

      <!-- Header Section -->
      <div class="absolute top-1 left-2 flex py-2 rounded-lg select-none" style="background-color: #11111133">
        <img src="logo.svg" alt="FrameworkZero logo" class="h-12" />
        <div class="pr-6 pt-0 m-0 ml-2 text-logo bright leading-tight self-center">
          <div class="text-xl -mt-1">FRAMEWORK</div>
          <div class="-mt-2 text-3xl">ZERO</div>
          <!-- Doc Version (logo) -->
          <div class="-mt-1 pl-1 text-sm text-red-500">v<span class="version-number"></span></div>
        </div>
      </div>

      <header class="text-center relative h-auto flex items-center justify-center py-9 mt-6 -mb-2">
        <section id="intro">
          <span class="menu-nav-target-title hidden">Introduction</span>
          <div class="relative z-10">
            <h1 class="text-logo font-extrabold uppercase leading-tight mb-2 text-white">
              <img src="logo.svg" alt="FrameworkZero logo" class="h-24 mx-auto mx-auto mb-3" />
              <span class="block text-xl md:text-2xl text-gray-300">International <span
                  class="text-nowrap">Hybrid-Technical</span></span>
              <span class="block text-3xl md:text-5xl">
                <span class="secondary">AI Safety </span>
                <span class="primary">Governance</span></span>
            </h1>
            <p class="text-xl md:text-2xl text-gray-400">A Foundational Framework for Frontier AI Safety Standards</p>
          </div>
        </section>
      </header>

      <!-- Intro Section -->
      <section id="intro" class="mb-16 section-hidden">
        <div class="content-card no-grad pt-0 p-6 md:p-10 md:pt-0 rounded-md text-lg md:text-xl">
          <div class="video-wrapper cursor-pointer relative flex justify-center items-center w-fit mx-auto -mx-10 pb-1">
            <video id="intro-video" autoplay muted playsinline preload="metadata" class="block opacity-50">
              <source src="flow.mp4" type="video/mp4" />
              <source src="flow.webp" type="video/webp" />
            </video>
            <img id="play-icon" src="playicon.svg"
              class="scale2x cursor-pointer absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2" />
            <h3 id="play-text"
              class="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2 -mt-9 ml-1 text-2xl font-bold text-shadow-2xl">
            </h3>
            <img src="landing.jpg" alt="FrameworkZero - International Hybrid-Technical AI Governance DAO"
              class="hidden mix-blend-screen" />
          </div>
          <p class="text-gray-300">

          <p class="text-center text-3xl font-bold accent bright">
            To provide the tools <span class="text-nowrap">and infrastructure</span><br>
            <span class="text-2xl">
              <span class="text-nowrap">to <i>actualize</i></span> <span class="text-nowrap">AI safety
                standards, globally.</span>
            </span>
          </p>
          <br>

          <p class="text-2xl">
          <span class="font-bold italic">AI safety is a global issue that implicitly requires international coordination.</span>    
          <br><br>
            An actionable international AI safety governance solution for frontier AI development utilizing
            collaborative verifiable safety, decentralized blockchain coordination, and automated technical 
            enforcement with meaningful human-in-the-loop orchestration.
          </p>
          <br>

          An open-source, not-for-profit non-cryptocurrency-based Decentralized Autonomous Organization (<span
            class="tp" data-tp="dao">DAO</span>) multilaterally developed
          and operated - enforced by nation states, adopted by the AI industry and integrated
          into AI-compute datacenters. 
          <br><br>
          Enabling global coordination between policy makers, safety experts and industry with tiered strong consensus
          voting,
          enabling dynamic adaptability and control in an evolving technical landscape.
          Setting beneficial red-lines to the race towards Artificial General Intelligence (<span class="tp"
            data-tp="agi">AGI</span>) while preventing critical AI risk.
          <br><br>
          Building on existing research in AI safety, AI governance, technical governance, compute governance, verifiable
          safety, specialized compute hardware (on/off chip), Trusted
          Execution Environments (<span class="tp" data-tp="tee">TEEs</span>) & Trusted
          Capable Model Environments (<span class="tp" data-tp="tcme">TCMEs</span>).
          </p>
        </div>
      </section>

      <hr class="hr-gradient bg-white my-16">

      <!-- Mission & Features Section -->
      <section id="mission" class="mb-16 section-hidden">
        <!-- Mission Statement Card -->
        <div class="content-card p-6 md:p-8 rounded-md">
          <h2 class="md:text-center text-left text-3xl font-bold mb-4 text-gradient bright menu-nav-target-title">
            Mission
            Statement</h2>
          <br>
          <div class="grid grid-cols-2 grid-rows-2 content-between place-items-center items-stretch gap-4 text-lg">
            <div class="text-xl font-bold col-span-2">To bring into being standardized safety-first global AI development.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Tone down dangerous national
              & market AI race dynamics.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Empower collaborative AI
              safety research and progress.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Establish agreed-upon red
              lines for frontier AI development.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Provide secure verifiable
              infrastructure & standardization.</div>
          </div>
        </div>
      </section>

      <section id="featuresAndProblems" class="grid grid-cols-1 lg:grid-cols-2 gap-8 md:gap-16 mb-16 section-hidden">

        <!-- Problems & Solutions Card -->
        <div class="content-card p-6 md:p-10 rounded-md">
          <h2 class="text-3xl font-bold mb-4 text-gradient bright menu-nav-target-title">
            Problems & Solutions</h2>
          <div class="gap-8 text-gray-300 text-lg">
            <div>
              <p><span class="font-bold">AI safety is either achieved globally or not at all.</span><br>
              <h3 class="text-2xl font-bold accent bright mb-2">Problems</h3>
              Establishing AI Safety Governance is fraught with:</p>
              <ul class="list-disc ml-4 space-y-2 pt-2">
                <li>Poor coordination across domains and stakeholders</li>
                <li>Geopolitical tensions and strategic vulnerabilities</li>
                <li>Difficulty in handling decentralized compute</li>
                <li>Overregulation that stifles innovation</li>
                <li>Delays in oversight and enforcement</li>
                <li>Double standards</li>
              </ul>
              <br>
              <h3 class="text-2xl font-bold accent bright mb-2">Solutions</h3>
              <span class="font-bold">Genuine coordination arises out of mutual need.</span>
              <ul class="list-disc ml-4 space-y-4 mt-2">
                <li><span class="italic font-bold">International:</span> Levels the playing field, setting race
                  rules to the of benefit all stakeholders.</li>
                <li><span class="italic font-bold">Hybrid-Technical:</span> Combines effective human oversight with robust
                  automated compliance.</li>
                <li><span class="italic font-bold">Blockchain <span class="tp" data-tp="dao">DAO</span></span> Offers
                  universal, transparent, collaborative oversight with implicit due diligence,
                  making it adversarial-hardened, verifiable and adaptable.</li>
              </ul>
              <br>
              <p>A strong solution necessitates multilateral government and intra-industry participation on AI development, AI-compute datacenters and
                specialized hardware.
                If it is to be accepted by stakeholders, the solution cannot be centralized or sovereign.
              </p>
              <br>
              <p><strong>This solution is not intended to supersede existing governance or AI lab safety.</strong>
                It provides the foundational layer allowing individual nations and AI labs to build off of for
                further AI governance, policy and safety.</p>
              <br>
              <h4 class="text-xl font-bold accent bright">Standardization</h4>
              <p>
                Establishing global standards allows global flourishing, providing the base level of confidence for
                pursuing AI applications across the sciences and economy. Importantly, it enables collaborative safety
                research.
              </p>
              <br>
              <h4 class="text-xl font-bold accent bright">Market Incentives</h4>
              <p>Participation grants access to otherwise
                restricted datacenters and cutting-edge safety tools which mitigate product blowback.
                <br>
                While it does not entirely replace AI lab safety infrastructures, it provides cost-effective compliance
                which reduces regulatory friction.
                It grants "certified trust" for AIs with standards that abate market stifling.
              </p>
            </div>
          </div>
        </div>

        <!-- Key Features Card -->
        <div class="content-card p-6 md:p-8 rounded-md">
          <h2 class="text-3xl font-bold mb-4 text-gradient bright menu-nav-target-title">Key Features</h2>
          <div class="space-y-4 text-gray-300 text-lg">
            <p><strong class="accent bright">Foundational Outer Alignment:</strong> Ensures AI aligns with
              bedrock polices set by international consensus.</p>
            <p><strong class="accent bright">Standardized Modular Safety:</strong>
              Embodies safety specs, policies, testing & capability red-lines which is applied based on an AI's specifics,
              (architecture, risk tier, declared intent, purpose and scope (narrow or general)).
              <br><br>
              It is comprised of <span class="tp" data-tp="precompute">Pre-Compute</span> and <span class="tp"
                data-tp="postcompute">Post-Compute</span> safety,
              consisting of voted-in safety research in the form of <span class="tp" data-tp="probe">Probes</span>
              and <span class="tp" data-tp="audit">Audits</span>.
              <br><br>
              Modular collaborative safety is achieved by
              <span class="tp" data-tp="template">templates</span>, standardizing framework components for Pre-Compute Safety Probes & Post-Compute Safety Audits,
              AI Architectures & compute Workload flows.
            </p>
            <p><strong class="accent bright">Compliance Enforcement:</strong> Automated mechanisms to enforce rules
              without constant human intervention, while preserving human authority.</p>
            <p><strong class="accent bright">Compute Governance:</strong>
              Decentralized handling of compute resources.
              Hardware verification and management (on/off-chip).
              Voted on compute budgets.
            </p>
            <p><strong class="accent bright">Training Safety Governance:</strong>
              Restrictions on training data domains (e.g. <span class="tp" data-tp="ccbrn">dangerous knowledge
                domains</span>). Adherence to standardized AI architectures, mitigating <span class="tp"
                data-tp="lossofcontrol">loss-of-control</span> risk.
            </p>
            <p><strong class="accent bright">AI Registry:</strong>
              Human oversight via voting met with automated technical safety.
              Privacy-preserving with verifiable cryptographic identity.
              AIs adhere to architecture templates, tier restrictions & declarations.
            </p>
            <p><strong class="accent bright">Personnel Registry:</strong>
              Registered and vetted <span class="tp" data-tp="personnel">personnel</span> across policy, safety,
              software & hardware.
              Includes a whistle-blower program for reporting issues.
            </p>
            <p><strong class="accent bright">Activity Logging:</strong> Logs all framework actions & events for
              transparent accountability.</p>
            <p><strong class="accent bright">Incentivized Adoption:</strong> Permitting otherwise restricted access to
              AI-compute datacenters (per national legal allowences), cost-effective
              safety infrastructure, reduced regulatory stifling and certified trust for AI systems/models.</p>
            <p><strong class="accent bright">Multilateral Voting:</strong>
              Tiered policy and safety personnel with distributed seats vote on AI registry, template
              designs, <span class="tp" data-tp="thestandard">the safety standards </span>, and framework architectural
              decisions.
            </p>
            <p><strong class="accent bright">Open-Source Transparency & Verifiable Trust:</strong> All components are
              auditable, adaptable and adversarial-hardened.
            </p>
          </div>
        </div>

      </section>

      <hr class="hr-gradient my-16">

      <!-- About Section -->
      <section id="about" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">
            About
          </h2>
          <p>
            <i>AI Safety FrameworkZero's</i> mission: <br>
            <strong>To bring into being standardized safety-first
              global AI development.</strong>
          </p>
          <br>
          <div class="block md:flex">
            <div class="basis-1/12 content-center">
              <img src="logo.svg" alt="FrameworkZero - International Hybrid-Technical AI Governance DAO"
                class="bg-center h-24" />
            </div>
            <div class="content-center basis-10/12 pl-0 md:pl-6 pr-24 pt-6 md:pt-0">
              It's ambition is to enact global coordination, inspire the international will and actualize an AI safety
              standard to prevent catastrophic AI risk.
              It sets forth an actionable plan should the nations come to realize AI safety can only truly be achieved as
              a global effort.<br>
              It is an open not-for-profit project.
            </div>
          </div>
          <br>
          <strong class="text-lg">This framework builds off existing research in AI safety and governance.</strong><br>
          Drawing heavily on The Oxford Martin AI Governance Initiative's research, specifically <a class="exlink"
            href="https://aigi.ox.ac.uk/publications/verification-for-international-ai-governance/"
            target="_blank">Harack, 2025</a>, <br>
          as well as
          <a class="exlink" href="https://arxiv.org/pdf/2506.23706" target="_blank">Schnabl, 2025</a>,
          <a class="exlink" href="https://arxiv.org/pdf/2501.08970" target="_blank">Shumailov, 2025</a>,
          <a class="exlink"
            href="https://intelligence.org/wp-content/uploads/2024/11/Mechanisms-to-Verify-International-Agreements-About-AI-Development-27-Nov-24.pdf"
            target="_blank">Scher, 2024</a>,
          <a class="exlink" href="https://arxiv.org/html/2405.06624v3" target="_blank">Dalrymple, 2024</a>,
          and the work of many others.
          <br>
          Inspired by organizations like LawZero & ControlAI, it is therefore <a class="exlink"
            href="https://www.narrowpath.co/" target="_blank">Narrow Path</a> compatible.
          <br>
          <br>
          <p>We are seeking collaborators. Feedback and criticisms are welcome.</p>
          <strong class="text-xl">Get in touch via the <a href="https://x.com/i/communities/1964271923119989164"
              target="_blank" class="exlink">ùïè community</a></strong>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <!-- Framework Architecture Section -->
      <section id="architecture" class="mb-16 section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-6 text-gradient bright menu-nav-target-title">Framework Architecture
          </h2>
          <p class="text-gray-300"><strong>Layered blockchain structure:</strong><br>
            <span class="accent bright"><span class="tp" data-tp="layer1">Layer-1</span></span> Mainnet controls core
            components which are <span class="tp" data-tp="sharding">sharded</span>.<br>
            <span class="accent bright"><span class="tp" data-tp="layer2">Layer-2</span></span> ZK-Rollups are employed
            for more dynamic components to reduce mainnet load until vote-actived.<br>
            <span class="accent bright"><span class="tp" data-tp="layer3">Layer-3</span></span> dApps provide user
            interfaces.
            <a href="architecture.png" target="_blank"><img src="architecture.png"
                class="py-6 contrast-125 mix-blend-lighten" /></a>
          </p>

          <p><strong class="accent bright"><span class="tp" data-tp="template">Templates</span></strong> for AI
            Architectures, training
            <span class="tp" data-tp="workload">Workloads</span>,
            <span class="tp" data-tp="precompute">Pre-Compute Safety</span>
            and <span class="tp" data-tp="postcompute">Post-Compute Safety</span> provide standardized protocols and
            better collaborative safety work.
            Critically important for AI architecture and compute workloads to enforce safe training, for mitigation
            of risks such as <span class="tp" data-tp="backdoor">backdoors</span>,
            recursive self-improvement (<span class="tp" data-tp="rsi">RSI</span>) and <span class="tp"
              data-tp="lossofcontrol">loss-of-control</span>.
          </p>
          <br>
          <p><strong class="accent bright">Blockchain <span class="tp" data-tp="layer2">Layer-2
                ZK-Rollups</span></strong> handle iterative development and voting for
            probes, audits, and templates - avoiding
            overload on the <span class="tp" data-tp="layer1">Layer-1</span> mainnet. Once voted in to The Safety Standards, these become immutable on
            Layer-1.
            They can only be removed/replaced via voting, ensuring adaptability while maintaining security
            and trust. As security is paramount, interactive zero-knowledge proofs are used over faster non-interactive
            zero-knowledge proof (NIZK) as well as quantum-resistant encryption.
          </p>
        </div>
      </section>

      <!-- Framework Flow Section -->
      <section id="flow" class="mb-16 section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">Framework Flow</h2>
          <p><strong>The framework serves to establish <span class="tp" data-tp="thestandard">the safety
                standards</span>,</strong>
            which consists of voted-upon standardized safety modules developed by the global community.
            <a href="safetyprocess.png" target="_blank"><img src="safetyprocess.png" alt="Safety Process Diagram"
                class="max-w-auto lg:max-w-4xl py-6 contrast-125 mix-blend-lighten" /></a>
          </p>
          <p>
            An registered AI attains certification after it has successfully passed the relevant safety standard on a temporary basis.
            Certifications must be renewed periodically, if it is requested via voting, or after any updates to the relevant
            safety standard.
            <a href="safetyflow.png" target="_blank"><img src="safetyflow.png" alt="Basic Safety Flow Diagram"
                class="pr-0 lg:pr-12 py-6 contrast-125 contrast-125 mix-blend-lighten" /></a>
          </p>
          <p class="text-lg text-gray-300">
            <strong>The framework flow governs safety with
              human-in-the-loop review,
              Pre-Compute Safety (<span class="tp" data-tp="probe">Probes</span>) and
              Post-Compute Safety (<span class="tp" data-tp="audit">Audits</span>). </strong> 
            The framework covers inference to a limited degree, essentially restricting on certification. This
            is due to inherent limitations in inference governance, as inference is less hardware restricted. 
            Certification further serves to catalog AIs, enabling regional groups to regulate usage of specific AI based on it's fingerprint.
            <br>
            <br>
            To mitigate AI situational awareness, classified (off-chain developed, encrypted on-chain) safety templates could be employed to effectively reduce AI situational
            awareness. This introduces the potential for discriminant application, enabling biased or sabotaging false-positives and false-negatives, 
            and would therefore entail adversarial development or assessment via Meta-Safety (Privacy-preserving TEE/TCME assessments on encrypted safety modules).
            <a href="flow.png" target="_blank"><img src="flow.png" alt="Framework Flow Diagram"
                class="max-w-auto lg:max-w-4xl py-6 contrast-125 mix-blend-lighten" /></a>
          </p>

          <div class="block md:flex px-6 gap-8">
            <ol class="flex-1 mt-2 list-inside space-y-4 text-gray-300">
              <li><strong class="text-lg accent bright">Initial AI Registration:</strong><br> AIs are registered
                with a 'Pending' status in which it must pass voting, where tiers/budgets are
                also assigned, before attaining 'Approved' status.
                <br><br>
                An AI can be registered at any <span class="tp" data-tp="phase">phase</span>, which
                allows pre-existing AIs to be onboarded. Onboarding an inference-ready AI must
                undergo Post-Compute Safety to be certified.
              </li>
              <li><strong class="text-lg accent bright">TEE-Deployed Container:</strong><br>
                After passing <span class="tp" data-tp="precompute">Pre-Compute Safety</span>,
                the AI begins training and it's <span class="tp"
                  data-tp="container">container</span> is loaded into a <span class="tp" data-tp="tee">TEE</span> for
                privacy-preserving execution of a <span class="tp" data-tp="workload">compute workload</span>, which are
                securely decrypted by AI authors at runtime.
                <br>
                <br>
                During execution the container is sealed & air-gapped (non-interactive with restricted networking).
                Once a Workload completes, a new 'Pending' inference <span class="tp" data-tp="assembly">AI</span> is created.
                <br>
              </li>
              <li><strong class="text-lg accent bright">Workload Run cycle:</strong><br>
                A <span class="tp" data-tp="workload">workload</span> consists of multiple runs conducted by the <span
                  class="tp" data-tp="container">container</span>,
                with each run specifying <span class="tp" data-tp="hardware">hardware</span> for compute.
                Each run (or run stage) depends on <span class="tp" data-tp="otk">OTKs</span> (required for
                hardware operation) tied to <span class="tp" data-tp="budget">compute budgets</span> and
                <span class="tp" data-tp="hwattestations">hardware integrity</span>. Runs can consist of multiple stages
                which is needed to handle variations in <span class="tp" data-tp="budget">phase</span> for hardware
                phase-locking.
                <br><br>
                Certified inference AIs have an
                alternative flow, the framework does not directly manage their workloads (input/output), only hardware compute (larger budget
                OTKs are issued).
                <br>
              </li>
              <li><strong class="text-lg accent bright">OTK Issuer:</strong><br> Uses near real-time hardware and model
                budgets for compute governance.
                OTKs are cryptographically generated on-chain, tied to a specific run, workload, model, and specific
                registered hardware, with expiry.<br>
                Semi-random OTK "drip" issues keys incrementally during the run, mitigating theft, reuse or
                decentralized compute gaming.
                <br><br>
                The <span class="tp" data-tp="otk">OTK</span> "drip" overcomes blockchain latency issues by issuing
                budget
                rations, potentially queuing an
                addition OTK to prevent interruption, never surpassing a run's (or hardware's) allotted budget.<br>
                <span class="tp" data-tp="budget">Budgets</span>
                are managed
                on-chain, updated throughout a run.
                The OTK Issuer halts the OTK "drip" on failed hardware integrity attestations.
                <br><br>
                Inference models are issued OTKs at less frequent increments with higher budget rations, tied to the
                underlying hardware.
              </li>
              <li><strong class="text-lg accent bright">Phase Detection:</strong><br>
                Hardware enforces a registered <span class="tp" data-tp="assembly">AI</span>'s (and its compute workload runs and run stages) <span class="tp" data-tp="phase">phase</span>
                to prevent phase
                gaming (e.g. Inference used for unapproved training).
                This is mitigated via architecture and workload <span class="tp" data-tp="template">templates</span>, in
                addition to <span class="tp" data-tp="precompute">Pre-Compute Safety</span>.
                <br>
              </li>
              <li>
                <strong class="text-lg accent bright">Hardware Verification:</strong><br> Registered on/off-chip
                specialized hardware
                (with approved budgets
                for compute, memory, etc.)
                must be physically verified, inspections are carried out via authorized registered <span class="tp"
                  data-tp="personnel">personnel</span> (or a group
                of adversarial personnel when
                requested), on a periodic basis.
              </li>
            </ol>
            <ol class="flex-1 mt-2 list-inside space-y-4 text-gray-300">
              <li>
                <span class="tp" data-tp="hwattestations">Integrity attestations</span> run semi-randomly during a run
                to detect
                tampering (e.g. BIOS modifications, physical seal breach, relocation, anomalies, etc). This makes
                gaming difficult by unpredictability and reduces on-chain load.
                <br><br>
                In the event of numerous failed hardware integrity attestations, the hardware is revoked. Revoked
                hardware re-acquires the status 'Pending' which initiates a physical inspection.
              </li>
              <li><strong class="text-lg accent bright">Safety Probes and Audits:</strong><br>
                <span class="tp" data-tp="probe">Probes</span> and <span class="tp"
                  data-tp="audit">Audits</span> are selected and approved via voting to establish <span
                  class="tp" data-tp="thestandard">The Safety Standards</span>, with different standards
                systematically applying depending on a registered <span class="tp" data-tp="assembly">AI</span>'s specifics.
                <br><br>
                <span class="tp" data-tp="precompute">Pre-Compute</span> Probes mitigate unauthorized
                data domains, banned architectures/algorithms, <span class="tp" data-tp="backdoor">backdoors</span>,
                <span class="tp" data-tp="rsi">RSI</span> detection and other violations. Depending on the severity level
                of a failed Probe, a workload may be cancelled and require re-registration.
                <span class="tp" data-tp="precompute">Post-Compute</span> <span class="tp" data-tp="audit">Audits</span> are comprised of evals, benches, automated red-teaming, 
                and other community safety research (e.g. Guaranteed Safe AI (<span class="tp" data-tp="gsai">GSAI)</span>).
                Both are run inside privacy-preserving <span class="tp" data-tp="tee">TEEs</span>
                or <span class="tp" data-tp="tcme">TCMEs</span>
                and produce reports without disclosing intellectual property details.
                <br><br>
                Audits detect violations, malicious capabilities and measure capability levels to enforce red-lines.
                They can limit general-purpose AI capabilities or restrict narrow AI to their domain.
              </li>
              <li><strong class="text-lg accent bright">Post-Workload:</strong><br>
                When training completes, a new Inference <span class="tp" data-tp="assembly">AI Assembly</span> is created with a 'Pending' status then subsequently run through
                multiple Audits (Post-Compute Safety). Audits with no scaffolding are run first, followed by 'live' Audits with
                full scaffolding (web access, tooling, etc).
                <br><br>
                If approved, the AI <span class="tp" data-tp="container">container</span> (post-compute training output) is
                transferred to off-chain storage (managed by AI authors), and can be used for inference.
                <br><br>
                Failing AIs are assigned a status of 'Rejected' and voted on to determine whether a model
                is permitted to undergo another attempt. If permitted, a new post-train model is registered with
                restrictive templates.
                <br><br>
                These restrictive templates enable post-training with proxy access to the failed model output, which is
                temporarily stored in a secure vault only accessible by the framework - this prevents exposure even to the
                AI authors.
              </li>
              <li><strong class="text-lg accent bright">Continual Learning Models:</strong><br>
                AI that <span class="tp" data-tp="continuallearning">continuously learns</span> outside of explicit
                training phases are handled via certification expiry.
                At each expiry, a new snapshot is registered as a separate inference <span class="tp" data-tp="assembly">AI Assembly</span> with a new expiry and is rerun through safety Audits.
              </li>
              <li><strong class="text-lg accent bright">Handling Inference:</strong><br>
                Inference only requires hardware <span class="tp" data-tp="otk">OTKs</span>, issued for registered certified AIs. This is the extent by which the
                framework directly governs inference.
                <br><br>
                Verification of certified inference AIs is done via the public cryptographic Container ID (fingerprint),
                allowing verification by regular datacenters, edge devices and other hardware with
                <span class="tp" data-tp="devicemating">model-device mating</span> - however enforcement is
                out-of-scope of the framework.
              </li>
            </ol>
          </div>
        </div>
      </section>

      <!-- Voting Section -->
      <section id="voting" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">Multilateral Voting
          </h2>
          <strong>Voting determines the policy and safety standards which are enforced by the framework.</strong><br>
          Participation happens via a tiered structure and is restricted to registered, vetted Policy & Safety
          <span class="tp" data-tp="personnel">personnel</span>.
          <br>
          <a href="safetyvoting.png" target="_blank"><img src="safetyvoting.png"
              class="max-w-auto lg:max-w-2xl contrast-125 mix-blend-lighten" /></a>
          <p>
            Ensuring perpetual safety entails continual update cycles, as it does with most cybersecurity.
            However, the transparent nature of this framework demands increased update frequency due to AI situational
            awareness.
            This is where AIs are aware of testing environments, often showing less harmful behavior during testing
            than in real-world deployments.
            This reinforces the need for certification renewals, as well as the critical importance of
            fully-tooled live audits.
          </p><br>
          <strong>Types of voting:</strong>
          <ul class="flex-1 mt-2 space-y-4 text-gray-300">
            <li><strong class="text-lg accent bright">Safety</strong><br>
              <ul class="list-disc ml-4">
                <li>Selecting safety modules for <span class="tp" data-tp="thestandard">the safety standard</span>,
                  which is a multiple-rounds process</li>
                <li>Approving new framework <span class="tp" data-tp="template">Templates</span>.
                </li>
              </ul>
            </li>
            <li><strong class="text-lg accent bright">Reviews</strong><br>
              <ul class="list-disc ml-4">
                <li>Approving new <span class="tp" data-tp="assembly">AI Assembly</span> registrations and new <span class="tp"
                    data-tp="hardware">hardware</span></li>
                <li>Onboarding existing models</li>
                <li>Setting <span class="tp" data-tp="budget">compute budgets</span> for AI Assemblies and hardware</li>
                <li>Allowing retries for post-workload safety failures</li>
                <li>Reviewing revoked hardware from failed <span class="tp" data-tp="hwattestations">integrity
                    attestations</span></li>
              </ul>
            </li>
            <li><strong class="text-lg accent bright">Framework</strong><br>
              <ul class="list-disc ml-4">
                <li><span class="tp" data-tp="token">Tokenomics</span> (e.g. gas price algorithm, gas tax and treasury
                  award)
                </li>
                <li>Tier changes for AI Assemblies and personnel</li>
              </ul>
            </li>
            <li><strong class="text-lg accent bright">Emergency</strong><br>
              <ul class="list-disc ml-4">
                <li>Threat escalation for lower tier reporting</li>
                <li><strong>Threat response:</strong>
                  <ul class="list-disc ml-4">
                    <li>Rejecting a model</li>
                    <li>Revoking hardware</li>
                    <li>Adding/removing safety modules</li>
                    <li>Compute budget penalties</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <section id="economy" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">Funding &
            Blockchain Economy
          </h2>
          <div class="text-gray-300">
            <strong class="text-2xl accent bright">Funding</strong><br>
            <strong>The development of the framework and its infrastructure is an international endeavor.</strong>
            <br>
            Financial support is sourced from:
            <ul class="text-lg text-gray-300 list-disc ml-4 mt-2 space-y-2">
              <li><strong>Nation States:</strong><br>
                Participating governments, initially superpowers, augmented by AI taxation at the discretion of each
                nation.</li>
              <li><strong>Supranational Organizations:</strong><br>
                Entities such as the UN, or regional alliances
                that prioritize cross-border coordination and governance.</li>
              <li><strong>AI Industry:</strong><br>
                Top industry leaders across frontier labs, infrastructure and hardware.</li>
            </ul>
            <br>
            <strong class="text-2xl accent bright">Blockchain Economy</strong><br>
            Computational framework activity is metered using gas tokens:
            <ul class="text-lg text-gray-300 list-disc ml-4 mt-2 space-y-2">
              <li><strong>Closed-Loop Resource Accounting</strong><br>
                Gas tokens are not a tradable cryptocurrency (DeFi) rather internal accounting units generated and
                managed by the framework itself.
                They function as a closed-loop "resource rationing" mechanism tied exclusively to framework operation.
              </li>
              <li><strong>Algorithmically Defined Pricing</strong><br>
                Gas costs are determined through algorithmic rules that can be updated via top tier policy voters. This
                ensures fairness, prevents volatility, and maintains economic predictability.
              </li>
            </ul>
            <br>
            Gas tokens measure resource use for:
            <ul class="text-lg text-gray-300 list-disc ml-4 mt-2 space-y-2">
              <li><strong>Blockchain Operations:</strong>
                <ul class="list-disc ml-4">
                  <li>Consensus mechanism transactions</li>
                  <li>Framework state updates</li>
                </ul>
              </li>
              <li><strong>Infrastructure Overheads</strong>
                <ul class="list-disc ml-4">
                  <li>Operational costs of safety mechanisms</li>
                  <li>Hardware inspection costs</li>
                  <li>Shared treasury funding</li>
                </ul>
              </li>
            </ul>
            <br>

            Tokens are purchased for model registration, safety module development, personnel registration, hardware
            registration, and nations to maintain the framework token pool.
            <br>
            <br>
            <p><strong>Model training and inference datacenter expenses</strong> (compute, electricity, cloud hosting,
              bandwidth)
              are not
              covered by gas tokens.
              Those are settled externally between datacenters and model registrants. This separation avoids
              over-delegating control to the framework, serving as an off-system security layer and preserving
              government regulation on datacenter customers.</p>
            <br>

            <p>
              <strong class="text-xl accent bright">Framework Token Pool for Upkeep</strong><br>
              A framework token pool is maintained multilaterally to cover operational costs, such as voting, safety module
              development, and certification renewals.
            </p>
            <br>

            <p>
              <strong class="text-xl accent bright">Shared Treasury for Incentivized Safety</strong><br>
              To incentivize on-framework safety research, a shared treasury is funded per-transaction (Gas tax) after an
              initial supply.
              For separation of concerns, an established cryptocurrency is used instead of the framework's tokens.
              <br><br>
              During the multi-round safety selection voting, which determines the specific safety modules used, the
              treasury
              awards winning modules for each round. Awards bounties are granted for the discovery of bugs and flaws in
              safety modules or the underlying framework itself. This mechanism supports existing AI safety organizations,
              compensating them for the release of intellectual property.
            </p>
          </div>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <section id="limitations" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">Limitations</h2>
          <ul class="text-lg text-gray-300 list-disc ml-4 space-y-4">
            <li><strong>Time</strong>, this solution assume longer timelines to reaching <span class="tp"
                data-tp="agi">AGI</span>.</li>
            <li><strong>Political will</strong> and recognition of the imminent need for collaborative international AI
              safety governance. <br>
              Nations therefore must become aware of critical AI risk and realize an AI weapons race benefits none.</li>
            <li><strong>Not designed for <span class="tp" data-tp="agi">AGI</span> or superintelligence;</strong>
              focuses on pre-AGI preventative red
              lines, as <strong>it is not a solution to AI alignment.</strong><br>
              Enduring governance of AGI or superintelligence is a fundamental problem in AI safety research.
              Leading AI scientists hold that it requires superalignment, a concept currently unsolved and possibly
              unsolvable given the timeline set forth by AI race.
            </li>
            <li><strong>Relies on specialized hardware development and cryptographic security.</strong><br>
              There is strong and growing research for the design of specialized hardware with some real-world
              implementation,
              however further development is needed.
              It is the ambition of this framework to enact global coordination and the will to accomplish this.
              <br><br>
              As frontier models grow in capability, so does their
              threat to cryptographic security, escalating the need of critical supervision and ongoing
              reinforcement. This framework's many security layers reduce risk impact along with the transparency for multilateral monitoring.
            </li>
            <li><strong>Limited insight into private model architectures.</strong><br>
              Model <span class="tp" data-tp="template">templates</span>,
              <span class="tp" data-tp="probe">Probes</span> and
              <span class="tp" data-tp="audit">Audits</span> reduce, yet do not
              eliminate, risks from dangerous model architectures.</strong>
            </li>
            <li><strong>Model lifecycle monitoring</strong> covers a model's phases and continual learning checkpoints.
              <br>
              Continual learning models include all paradigms of online/streaming continual/lifelong‚Äëlearning systems
              (e.g. real‚Äëtime/frequent reinforcement‚Äëlearning) or with any form of meta-learning (short of RSI).
              For such models, this framework has limited control, and
              likely inadequate for more advanced models of this category. Preventative measures and mitigation
              are achieved by architecture templating, probes, audits, and voting revision calls.
            </li>
            <li><strong>Off-framework training remains possible,</strong> though heavily hindered by global mandates on
              datacenters. Frontier AI advancements could produce model architectures that require minimal compute for
              training, running on standard hardware.</li>
            <li><strong>Does not fully govern model inference.</strong><br>
              While inference at participating AI-compute datacenters can only happen with approved, certified models,
              governance of inference input/output is beyond the scope of the framework.
              On-chain framework safeguards for inference input/output could be added to this framework,
              though enforcement of use would be out-of-scope.
              <br><br>
              Inference requires fast and low-latency processing at scale, which conflicts with the slower and
              resource-intensive nature of blockchain.
              Neither does inference necessitate AI-compute hardware, with current many frontier AIs already
              runnable on edge devices - a trend likely to hold as AI and hardware continues to advance.
            </li>
          </ul>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <!-- Detailed Mechanisms Section -->
      <section id="mechanisms" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">
            Appendix
          </h2>
          <ul class="list-inside space-y-12 text-gray-300">

            <li id="apdx-tee"><strong class="text-xl accent bright">Trusted Execution Environments (TEEs):</strong><br>
              TEEs provide secure and privacy-preserving code execution in a sealed virtual environment.
              They allow the framework to securely deploy <span class="tp" data-tp="assembly">AI</span> containers during safety and training. They produce reports without
              disclosing intellectual property details. They are utilized for Pre/Post-safety and during hardware compute.
              <br>
              <a class="exlink" href="https://arxiv.org/pdf/2506.23706" target="_blank">Schnabl, 2025</a>
            </li>

            <li id="apdx-tcme"><strong class="text-xl accent bright">Trusted Capable Model Environments
                (TCMEs):</strong><br>
              TCMEs allow a stateless AI model to be instructed to privately verify model container and workload code to
              detect
              banned patterns and red-line violations (e.g. RSI, unapproved architecture, or dangerous algorithms).
              These Trusted Capable Models are 'trusted' in that they act as a neutral mediating party with agreed
              objective (set of instructions) and output.
              They operate within a sealed environment, outputting a verification report without disclosing
              intellectual
              property details.
              <br>
              <br>
              They are not infallible as they are limited by the confidence and capabilities of the
              underlying
              models (Capable Models).
              Despite this, they are an invaluable tool to explore closed-source code bases and
              private post-Workload output.
              <br>
              <a class="exlink" href="https://arxiv.org/pdf/2501.08970" target="_blank">Shumailov, 2025</a>
            </li>

            <li id="apdx-gsai"><strong class="text-xl accent bright">Guaranteed Safe AI (GSAI):</strong><br>
              A framework that provides quantifiable guarantees of AI safety, encapsulated an Audit.
              <br><br>
              "Approaches that follow the GSAI framework aim to provide the level of quantitative safety guarantees
              we've
              come to expect from other engineered systems. This is achieved with three core components: an auditable,
              separable world model, a way to describe what portions of the state space are 'safe' and 'unsafe', and a
              verifier (which provides an auditable proof certificate that the output satisfy the safety specification
              in the world model)." - <a class="exlink" href="https://www.gsais.org/#gsai" target="_blank">from
                GSAIS.org</a>
            </li>

            <li id="apdx-rsi"><strong class="text-xl accent bright">Recursive Self-Improvement (RSI)
                Mitigation</strong><br>
              RSI is mitigated through (A) <span class="tp" data-tp="assembly">AI</span> registration
              (slowing releases), (B) architecture/workload templates (C) pre-compute safety and (D) compute budgets.
              It is important to note that total elimination of these concerns is not feasible given the development of
              novel architectures, paradigms and algorithmic improvements.
            </li>

            <li id="apdx-phase"><strong class="text-xl accent bright">Phase Detection:</strong><br>
              Reliably detecting a model's Phase requires adjustments, and may even become infeasible as architectures
              and hardware advance. The variables for determining phase are: 
              Compute, Energy Use, Data size, Precision, Memory Usage, Batch Size, Latency Focus, Accelerator Use, Workload Type, Training Duration and Output.
            </li>
          </ul>
        </div>
      </section>


    </div>
    <!-- Footer -->
    <section class="section-hidden relative bottom-0 left-0 right-0 z-10">
      <hr class="hr-gradient">
      <div class="p-4 bg-zinc-900 text-lg text-center tracking-wide">
        AI Safety <span class="font-bold">FrameworkZero.org</span> Project
        <span class="mx-2">|</span>
        <span class="italic">Version <span class="version-number"></span></span>
        <span class="mx-2">|</span>
        2025
      </div>
    </section>
  </div>

  <script>
    // Versioning
    const versionNumber = "1.10"
    window.addEventListener('load', () => {
      const versionElements = document.getElementsByClassName('version-number')
      Array.from(versionElements).forEach(e => e.textContent = versionNumber)
    })
  </script>

</body>

</html>