<!DOCTYPE html>
<html lang="en" class="scroll-smooth">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="fav.svg" type="image/x-icon">
  <title>FrameworkZero</title>
  <meta property="og:image" content="landing.jpg" />
  <meta property="og:image:width" content="600" />
  <meta property="og:image:height" content="300" />
  <meta property="og:image:alt" content="FrameworkZero - International Hybrid-Technical AI Governance DAO" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link rel="preload" as="font" href="https://fonts.gstatic.com/s/sansation/v1/LYjFdGPjnEg8DNA0z01YEV8PVA.ttf"
    type="font/ttf" crossorigin="anonymous" />
  <link href="https://fonts.googleapis.com/css2?family=Sansation:wght@700&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Flex:opsz,wght@8..144,400;8..144,700&display=swap"
    rel="stylesheet">
  <link href="style.min.css" rel="stylesheet">
  <script src="scripts.js" defer async></script>
</head>

<body class="bg-gray-950 text-gray-200 subpixel-antialiased">

  <!-- Canvas for the background animation -->
  <canvas id="header-canvas" class="fixed left-0 w-full h-75 z-0"></canvas>

  <div id="tp-container" class="invisible fixed bottom-0 left-0 right-0 py-6 px-6 md:px-24 z-50
      text-base text-center text-white border-t-2 secondary-b
      max-h-36 overflow-y-auto shadow-lg">
  </div>
  <hr class="fixed bottom-0 left-0 right-0 hr-gradient h-2 z-40">

  <div id="main">

    <!-- Context Menu -->
    <nav class="context-menu">
      <div class="menu-button" id="menu-button">
        <!-- New menu icon as requested -->
        <svg class="menu-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1"
          stroke-linecap="round" stroke-linejoin="round">
          <path d="M12 2L2 7l10 5 10-5-10-5z"></path>
          <path d="M2 17l10 5 10-5M2 12l10 5 10-5"></path>
        </svg>
      </div>
      <div class="menu-list-container" id="menu-list">
        <!-- Doc Version (menu) -->
        <div class="mt-1 ml-4 select-none text-red-400">Version <span class="version-number"></span></div>
        <!-- Menu items will be dynamically populated -->
        <ul></ul>
      </div>
    </nav>

    <div class="container max-w-6xl mx-auto px-4 py-12 md:px-6">

      <!-- Header Section -->
      <div class="absolute top-1 left-2 flex py-2 rounded-lg select-none" style="background-color: #11111133">
        <img src="logo.svg" alt="FrameworkZero logo" class="h-12" />
        <div class="pr-6 pt-0 m-0 ml-2 text-logo bright leading-tight self-center">
          <div class="text-xl -mt-1">FRAMEWORK</div>
          <div class="-mt-2 text-3xl">ZERO</div>
          <!-- Doc Version (logo) -->
          <div class="-mt-1 pl-1 text-sm text-red-500">v<span class="version-number"></span></div>
        </div>
      </div>

      <header class="text-center relative h-auto flex items-center justify-center py-9 mt-6 mb-4">
        <section id="intro">

          <span class="menu-nav-target-title hidden">Introduction</span>
          <div class="relative z-10">
            <h1 class="text-logo font-extrabold uppercase leading-tight mb-2 text-white">
              <span class="block text-xl md:text-3xl text-gray-300">International <span
                  class="text-nowrap">Hybrid-Technical</span></span>
              <span class="block text-4xl md:text-6xl">
                <span class="secondary">AI Governance </span>
                <span class="primary">DAO</span></span>
            </h1>
            <p class="text-xl md:text-2xl text-gray-400">A foundational framework for responsible AI development</p>
          </div>
        </section>
      </header>

      <!-- Intro Section -->
      <section id="intro" class="mb-16 section-hidden">
        <div class="content-card intro-card p-6 md:p-10 rounded-md text-lg md:text-xl">
          <div class="flex w-full place-content-center -mt-8">
            <img src="landing.jpg" alt="FrameworkZero - International Hybrid-Technical AI Governance DAO"
              class="mix-blend-screen" />
          </div>
          <div class="text-center">
            <div class="text-3xl font-bold">To provide the tools <span class="text-nowrap">and infrastructure</span>
            </div>
            <div class="text-2xl font-bold">to <i>actualize</i> <span class="text-nowrap">a global AI safety
                standard</span></div>
          </div>
          <br>
          <p class="text-gray-300">
            <span class="text-2xl">
              An actionable international AI governance solution utilizing blockchain, collaborative
              verifiable safety, and automated technical enforcement with human-in-the-loop orchestration.
            </span>
            <br><br>
            An open-source blockchain-based Decentralized Autonomous Organization (<span class="tp"
              data-tp="dao">DAO</span>) multilaterally developed
            and operated - enforced by nation states, adopted by the AI industry and integrated
            in to AI-compute datacenters.
            <br><br>
            Enabling global coordination between policy makers, safety experts and industry with tiered voting,
            enabling dynamic adaptability and control in an evolving technical landscape.
            Setting red-lines to the race towards Artificial General Intelligence (<span class="tp"
              data-tp="agi">AGI</span>) while preventing
            critical AI risk.
            <br><br>
            Building on existing research in AI governance, technical governance, compute governance, verifiable
            safety, specialized compute hardware (on/off chip), Trusted
            Execution Environments (<span class="tp" data-tp="tee">TEEs</span>) & Trusted
            Capable Model Environments (<span class="tp" data-tp="tcme">TCMEs</span>).
          </p>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <!-- Mission & Features Section -->
      <section id="mission" class="mb-16 section-hidden">
        <!-- Mission Statement Card -->
        <div class="content-card p-6 md:p-8 rounded-md">
          <h2 class="md:text-center text-left text-3xl font-bold mb-4 text-gradient bright menu-nav-target-title">
            Mission
            Statement</h2>
          <br>
          <div class="grid grid-cols-2 grid-rows-2 content-between place-items-center items-stretch gap-4 text-lg">
            <div class="text-xl font-bold col-span-2">To bring into being standardized
              safety-first global AI development that is mutually advantageous.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Tone down dangerous national
              & market AI race dynamics.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Empower collaborative AI
              safety research and progress.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Establish agreed-upon red
              lines for frontier AI development.</div>
            <div class="accent-b border py-4 px-4 w-full rounded col-span-2 md:col-span-1">Provide secure verifiable
              infrastructure & standardization.</div>
          </div>
        </div>
      </section>

      <section id="featuresAndProblems" class="grid grid-cols-1 lg:grid-cols-2 gap-8 md:gap-16 mb-16 section-hidden">

        <!-- Problems & Solutions Card -->
        <div class="content-card p-6 md:p-10 rounded-md">
          <h2 class="text-3xl font-bold mb-4 text-gradient bright menu-nav-target-title">Problems &
            Solutions</h2>
          <div class="gap-8 text-gray-300 text-lg">
            <div>
              <h3 class="text-2xl font-bold accent bright">Problems</h3>
              <p><span class="font-bold">AI safety is a global issue and priority.</span><br>
                Establishing AI Governance is fraught with:</p>
              <ul class="list-disc ml-4 space-y-2 pt-2">
                <li>Poor coordination across domains and stakeholders.</li>
                <li>Geopolitical tensions and strategic vulnerabilities.</li>
                <li>Difficulty in handling decentralized compute.</li>
                <li>Overregulation that stifles innovation.</li>
                <li>Delays in oversight and enforcement.</li>
              </ul>
              <br>
              <h3 class="text-2xl font-bold accent bright">Solutions</h3>
              <span class="font-bold">Genuine coordination arises out of mutual need.</span>
              <ul class="list-disc ml-4 space-y-4 mt-2">
                <li><span class="italic font-bold">International:</span> Levels the playing field, setting race
                  rules to the of benefit all stakeholders.</li>
                <li><span class="italic font-bold">Hybrid-Technical:</span> Combines human oversight with robust
                  automated compliance.</li>
                <li><span class="italic font-bold">Blockchain <span class="tp" data-tp="dao">DAO</span></span> Offers
                  universal, transparent, collaborative
                  oversight with implicit
                  due
                  diligence,
                  making it adversarial-hardened, verifiable and adaptable. Alleviates the burden of trust.</li>
              </ul>
              <br>
              <p>A strong solution necessitates multilateral government participation on AI-compute datacenters and
                specialized hardware.
                If it is to be accepted by adversarial nations or organizations, the solution cannot be sovereign,
                centralized or forced.
              </p>
              <br>
              <p><strong>This solution is not intended to replace existing governance rather to augment it.</strong>
                It
                provides the foundational layer allowing individual nations and organizations to build on top for
                further AI governance, policy and safety.</p>
              <br>
              <h4 class="text-xl font-bold accent bright">Standardization</h4>
              <p>
                Establishing global standards allows global flourishing, providing the base level of confidence for
                pursuing AI applications across the sciences and economy. Importantly, it enables collaborative safety
                research,
                which is either achieved worldwide or not at all.
              </p>
              <br>
              <h4 class="text-xl font-bold accent bright">Market Incentives</h4>
              <p>Participation grants access to otherwise
                restricted datacenters and state-of-the-art safety tools which mitigate product blowback.
                <br>
                While it does not entirely replace AI lab safety infrastructures, it provides cost-effective compliance
                reducing regulatory friction.
                It grants "certified trust" for models with standards that abate market stifling.
              </p>
            </div>
          </div>
        </div>

        <!-- Key Features Card -->
        <div class="content-card p-6 md:p-8 rounded-md">
          <h2 class="text-3xl font-bold mb-4 text-gradient bright menu-nav-target-title">Key Features</h2>
          <div class="space-y-4 text-gray-300 text-lg">
            <p><strong class="accent bright">Foundational Outer Alignment:</strong> Ensures AI aligns with
              bedrock polices set by international consensus.</p>
            <p><strong class="accent bright">Standardized Modular Safety:</strong>
              Embodies safety specs, policies, testing & capability red-lines which is applied based on a model's
              architecture,
              risk tier, declared intent, purpose and scope (narrow or general).
              <br><br>
              It is comprised of verifiable <span class="tp" data-tp="guardrail">guardrails</span> and <span class="tp"
                data-tp="audit">audits</span>,
              consisting of voted-in safety research in the form of <span class="tp" data-tp="guard">guards</span>
              and <span class="tp" data-tp="check">checks</span>.
              <br><br>
              Modular collaborative safety is achieved by
              <span class="tp" data-tp="template">templates</span>, standardizing system components such as guards,
              checks, permitted model architectures & workload flows.
            </p>
            <p><strong class="accent bright">Compliance Enforcement:</strong> Automated mechanisms to enforce rules
              without constant human intervention, while preserving human authority.</p>
            <p><strong class="accent bright">Compute Governance:</strong>
              Decentralized handling of compute resources.
              Hardware verification and management (on/off-chip).
              Voted on compute budgets.
            </p>
            <p><strong class="accent bright">Training Run Governance:</strong>
              Restrictions on training data domains (e.g. <span class="tp" data-tp="ccbrn">dangerous knowledge
                domains</span>). Adherence to standardized AI model architectures, mitigating <span class="tp"
                data-tp="lossofcontrol">loss-of-control</span> risk.
            </p>
            <p><strong class="accent bright">Model Registry:</strong>
              Human oversight via voting met with automated technical safety.
              Privacy-preserving with verifiable cryptographic identity.
              Models adhere to architecture templates, tier restrictions & declared purpose.
            </p>
            <p><strong class="accent bright">Personnel Registry:</strong>
              Registered and vetted <span class="tp" data-tp="personnel">personnel</span> across policy, safety,
              software & hardware.
              Includes a whistle-blower program for reporting issues.
            </p>
            <p><strong class="accent bright">Activity Logging:</strong> Logs all system actions & events for
              transparent accountability.</p>
            <p><strong class="accent bright">Incentivized Adoption:</strong> Permitting otherwise restricted access to
              AI-compute datacenters, cost-effective
              safety infrastructure, reduced regulatory stifling and certified trust for AI models.</p>
            <p><strong class="accent bright">Multilateral Voting:</strong>
              Tiered policy and safety personnel with distributed seats vote on model registry, template
              designs, <span class="tp" data-tp="thestandard">the safety standard </span>, and framework architectural
              decisions.
            </p>
            <p><strong class="accent bright">Open-Source Transparency & Verifiable Trust:</strong> All components are
              auditable, adaptable and adversarial-hardened.
            </p>
          </div>
        </div>

      </section>

      <hr class="hr-gradient my-16">

      <div class="text-center">
        <h2 class="text-5xl font-bold">Technical Details</h2>
      </div>
      <hr class="hr-gradient my-16">

      <!-- System Architecture Section -->
      <section id="architecture" class="mb-16 section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-6 text-gradient bright menu-nav-target-title">System Architecture
          </h2>
          <p class="text-gray-300"><strong>Layered blockchain structure:</strong><br>
            <span class="accent bright"><span class="tp" data-tp="layer1">Layer-1</span></span> Mainnet controls core
            components which are <span class="tp" data-tp="sharding">sharded</span>
            accordingly.<br>
            <span class="accent bright"><span class="tp" data-tp="layer2">Layer-2</span></span> ZK-Rollups are employed
            for less dynamic components to reduce
            mainnet load.<br>
            <span class="accent bright"><span class="tp" data-tp="layer3">Layer-3</span></span> dApps provide user
            interfaces.
            <a href="architecture.png" target="_blank"><img src="architecture.png" class="py-6" /></a>
          </p>

          <p><strong class="accent bright"><span class="tp" data-tp="template">Templates</span></strong> for AI model
            architectures,
            <span class="tp" data-tp="workload">workloads</span>,
            <span class="tp" data-tp="guardrail">guardrails</span>
            and <span class="tp" data-tp="audit">audits</span> provide standardized protocols and
            better collaborative safety work.
            Critically important for Model architecture and Workloads to enforce safe training paradigms, for mitigation
            of risks such as <span class="tp" data-tp="backdoor">backdoors</span>,
            recursive self-improvement (<span class="tp" data-tp="rsi">RSI</span>) and <span class="tp"
              data-tp="lossofcontrol">loss-of-control</span>.
          </p>
          <br>
          <p><strong class="accent bright">Blockchain <span class="tp" data-tp="layer2">Layer-2
                ZK-Rollups</span></strong> handles authoring and voting of
            audits,
            guardrails and templates that change less frequently, avoiding
            overload on <span class="tp" data-tp="layer1">Layer-1</span>. Once voted on, these become immutable on
            Layer-1 for active integration.
            They can be removed/replaced only through voting, ensuring adaptability while maintaining security
            and trust. As security is paramount, interactive zero-knowledge proofs are used over faster non-interactive
            zero-knowledge proof (NIZK).
          </p>
        </div>
      </section>

      <!-- System Flow Section -->
      <section id="flow" class="mb-16 section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">System Flow</h2>
          <p><strong>The framework serves to establish a <span class="tp" data-tp="thestandard">safety
                standard</span>,</strong>
            which consists of voted-upon standardized safety modules developed by the global community.
            <a href="safetyprocess.png" target="_blank"><img src="safetyprocess.png" alt="Safety Process Diagram"
                class="max-w-auto lg:max-w-4xl py-6" /></a>
          </p>
          <p>
            An AI model
            attains certification after it has successfully passed the safety standard.
            Certified models must be renewed periodically, if it is requested via voting, and after any updates to the
            safety standard.
            <a href="safetyflow.png" target="_blank"><img src="safetyflow.png" alt="Basic Safety Flow Diagram"
                class="pr-0 lg:pr-12 py-6" /></a>
          </p>
          <p class="text-lg text-gray-300">
            <strong>The system flow governs models with
              human-in-the-loop review,
              pre-safety (<span class="tp" data-tp="guardrail">guardrails</span>) and
              post-safety (<span class="tp" data-tp="audit">audits</span>).</strong><br>
            The system covers inference to a limited degree, essentially serving as model certification. This
            is due to inherent limitations in inference governance, as inference is less hardware restricted.
            <a href="flow.png" target="_blank"><img src="flow.png" alt="System Flow Diagram"
                class="max-w-auto lg:max-w-4xl py-6" /></a>
          </p>

          <div class="block md:flex px-6 gap-8">
            <ol class="flex-1 mt-2 list-inside space-y-4 text-gray-300">
              <li><strong class="text-lg accent bright">Initial Model Registration:</strong><br> Models are registered
                with a 'Pending'
                status in which it must pass voting, where tiers/budgets are
                also assigned, before attaining 'Approved' status.
                <br><br>
                A model can be registered at any <span class="tp" data-tp="phase">phase</span>, which
                allows pre-existing models (or open-weights) to be onboarded. Onboarding an inference-ready model must
                undergo post-safety.
              </li>
              <li><strong class="text-lg accent bright">TEE-Deployed Container:</strong><br>
                After passing pre-safety (<span class="tp" data-tp="guardrail">guardrails</span>), a
                training <span class="tp" data-tp="model">model</span> and <span class="tp"
                  data-tp="container">container</span> are loaded into a <span class="tp" data-tp="tee">TEE</span> for
                privacy-preserving execution of a <span class="tp" data-tp="workload">workload</span>, which are
                securely decrypted by model authors at runtime.
                <br>
                <br>
                During execution the container is sealed & air-gapped (non-interactive with restricted system network).
                Once a Workload completes, Runs are assembled and a new 'Pending' inference <span class="tp"
                  data-tp="model">model</span> is
                registered.
                <br>
              </li>
              <li><strong class="text-lg accent bright">Workload Run cycle:</strong><br>
                A <span class="tp" data-tp="workload">workload</span> consists of multiple runs conducted by the <span
                  class="tp" data-tp="container">container</span>,
                with each run specifying <span class="tp" data-tp="hardware">hardware</span> for compute.
                Each run (or run stage) depends on <span class="tp" data-tp="otk">OTKs</span> (required for
                hardware operation) tied to <span class="tp" data-tp="budget">compute budgets</span> and
                <span class="tp" data-tp="hwattestations">hardware integrity</span>. Runs can consist of multiple stages
                which is needed to handle variations in <span class="tp" data-tp="budget">phase</span> for hardware
                phase-locking.
                <br><br>
                Certified inference models have an
                alternative flow, the system does not directly manage their workloads (input/output) and larger budget
                OTKs are issued.
                <br>
              </li>
              <li><strong class="text-lg accent bright">OTK Issuer:</strong><br> Uses near real-time hardware and model
                budgets for compute governance.
                OTKs are cryptographically generated on-chain, tied to a specific run, workload, model, and specific
                registered hardware, with expiry.<br>
                Semi-random OTK "drip" issues keys incrementally during the run, mitigating theft, reuse or
                decentralized compute gaming.
                <br><br>
                The <span class="tp" data-tp="otk">OTK</span> "drip" overcomes blockchain latency issues by issuing
                budget
                rations, potentially queuing an
                addition OTK to prevent interruption, never surpassing a run's (or hardware's) allotted budget.<br>
                <span class="tp" data-tp="budget">Budgets</span>
                are managed
                on-chain, updated throughout a run.
                The OTK Issuer halts the OTK "drip" on failed hardware integrity attestations.
                <br><br>
                Inference models are issued OTKs at less frequent increments with higher budget rations, tied to the
                underlying hardware.
              </li>
              <li><strong class="text-lg accent bright">Phase Detection:</strong><br>
                Hardware enforces a model (or workload runs or run stages) <span class="tp" data-tp="phase">phase</span> to prevent phase
                gaming (e.g.
                Inference used for unapproved training).
                This is mitigated via model and workload <span class="tp" data-tp="template">templates</span>, in
                addition to <span class="tp" data-tp="guardrails">guardrails</span>.
                <br>
                <a class="underline" href="#apdx-phase">See appendix for phase detection specifics.</a>
              </li>
              <li>
                <strong class="text-lg accent bright">Hardware Verification:</strong><br> Registered on/off-chip
                specialized hardware
                (with approved budgets
                for compute, memory, etc.)
                must be physically verified, inspections are carried out via authorized registered <span class="tp"
                  data-tp="personnel">personnel</span> (or a group
                of adversarial personnel when
                requested), on a periodic basis.
              </li>
            </ol>
            <ol class="flex-1 mt-2 list-inside space-y-4 text-gray-300">
              <li>
                <span class="tp" data-tp="hwattestations">Integrity attestations</span> run semi-randomly during a run
                to detect
                tampering (e.g. BIOS modifications, physical seal breach, relocation, anomalies, etc). This makes
                gaming difficult by unpredictability and reduces on-chain load.
                <br><br>
                In the event of numerous failed hardware integrity attestations, the hardware is revoked. Revoked
                hardware re-acquires the status 'Pending' which initiates a physical inspection.
              </li>
              <li><strong class="text-lg accent bright">Guardrails and Audits:</strong><br>
                <span class="tp" data-tp="audit">Audits</span> and <span class="tp"
                  data-tp="guardrail">guardrails</span> are selected and approved via voting to establish <span
                  class="tp" data-tp="thestandard">the safety standards</span>, with different sets
                applying depending on the model template used.
                <br><br>
                <span class="tp" data-tp="guardrail">Guardrails</span> mitigate unauthorized
                data domains and architectures, in addition to <span class="tp" data-tp="backdoor">backdoors</span> and
                <span class="tp" data-tp="rsi">RSI</span> detection. Depending on the severity level
                of a failed guardrail, a workload may be cancelled.
                Audits verify safety via community research <span class="tp" data-tp="check">checks</span>.
                Both are run inside privacy-preserving <span class="tp" data-tp="tee">TEEs</span> 
                or <span class="tp" data-tp="tcme">TCMEs</span>
                and produce reports without disclosing intellectual property details.
                <br><br>
                Audits measure capability levels and are used to enforce universal, as well as model-based, red-lines,
                serving to limit general-purpose models or restrict narrow AI models to their domain.
                They encapsulate community created concepts (e.g. evaluations, benchmarks, and Guaranteed Safe AI (<span class="tp" data-tp="gsai">GSAI))</span>.
              </li>
              <li><strong class="text-lg accent bright">Post-Workload:</strong><br>
                When a workload completes, a new 'Pending' Inference model is created then subsequently run through
                multiple audits (post-safety). Audits with no scaffolding are run first, followed by 'live' audits with
                full
                scaffolding (web calls, tooling, etc).
                <br><br>
                If approved, the model <span class="tp" data-tp="container">container</span> (post-workload output) is
                transferred to off-chain storage (managed by model authors), and
                can be used for inference.
                <br><br>
                Failing models are assigned a status of 'Rejected' and voted on to determine whether a model
                is permitted to undergo another attempt. If permitted, a new post-train model is registered with
                restrictive templates.
                <br><br>
                These restrictive templates enable post-training with proxy access to the failed model output, which is
                temporarily stored in a secure vault only accessible by the system - this prevents exposure even to the
                model authors.
              </li>
              <li><strong class="text-lg accent bright">Continual Learning Models:</strong><br>
                Models that <span class="tp" data-tp="continuallearning">continuously learn</span> outside of explicit
                training phases are handled via model expiry.
                At each expiry, a checkpoint model is registered as new inference model with a new expiry.
              </li>
              <li><strong class="text-lg accent bright">Inference at AI-compute datacenters:</strong><br>
                Inference only requires <span class="tp" data-tp="otk">OTKs</span>, reliant on a certified inference
                model. This is the extent by which the
                system directly governs inference.
                <br><br>
                Verification of certified inference models with the cryptographic Container ID (fingerprint) is openly
                available,
                allowing verification by regular datacenters, edge devices and other hardware with
                <span class="tp" data-tp="devicemating">model-device mating</span> - however enforcement is
                out-of-scope of the system.
              </li>
            </ol>
          </div>
        </div>
      </section>

      <!-- Voting Section -->
      <section id="voting" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">Multilateral Voting
          </h2>
          <strong>Voting determines the policy and safety standards which are enforced by the system.</strong><br>
          Participation happens via a tiered structure and is restricted to registered, vetted Policy & Safety
          <span class="tp" data-tp="personnel">personnel</span>.
          <br>
          <a href="safetyvoting.png" target="_blank"><img src="safetyvoting.png" class="max-w-auto lg:max-w-2xl" /></a>
          <p>
            Ensuring perpetual safety entails continual update cycles, as it does with most cybersecurity.
            However, the transparent nature of this framework demands increased update frequency due to AI situational
            awareness.
            This is where AI model are aware of testing environments, often showing less harmful behavior during testing
            than in real-world deployments.
            This reinforces the need for models to undergo certification renewals, as well as the critical importance of
            fully-tooled live audits.
          </p><br>
          <strong>Types of voting:</strong>
          <ul class="flex-1 mt-2 space-y-4 text-gray-300">
            <li><strong class="text-lg accent bright">Safety</strong><br>
              <ul class="list-disc ml-4">
                <li>Selecting safety modules for <span class="tp" data-tp="thestandard">the safety standard</span>,
                  which is a multiple-rounds process</li>
                <li>Approving new <span class="tp" data-tp="template">templates</span> for models, audits and guardrails
                </li>
              </ul>
            </li>
            <li><strong class="text-lg accent bright">Reviews</strong><br>
              <ul class="list-disc ml-4">
                <li>Approving new <span class="tp" data-tp="model">model</span> training and new <span class="tp"
                    data-tp="hardware">hardware</span></li>
                <li>Onboarding existing models</li>
                <li>Setting <span class="tp" data-tp="budget">compute budgets</span> for models and hardware</li>
                <li>Allowing retries for post-workload safety failures</li>
                <li>Reviewing revoked hardware from failed <span class="tp" data-tp="hwattestations">integrity
                    attestations</span></li>
              </ul>
            </li>
            <li><strong class="text-lg accent bright">System</strong><br>
              <ul class="list-disc ml-4">
                <li><span class="tp" data-tp="token">Tokenomics</span> (e.g. gas price algorithm, gas tax and treasury
                  award)
                </li>
                <li>Tier changes for AI models and personnel</li>
              </ul>
            </li>
            <li><strong class="text-lg accent bright">Emergency</strong><br>
              <ul class="list-disc ml-4">
                <li>Threat escalation for lower tier reporting</li>
                <li><strong>Threat response:</strong>
                  <ul class="list-disc ml-4">
                    <li>Rejecting a model</li>
                    <li>Revoking hardware</li>
                    <li>Adding/removing safety modules</li>
                    <li>Compute budget penalties</li>
                  </ul>
                </li>
              </ul>
            </li>
          </ul>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <section id="economy" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">Funding &
            Blockchain Economy
          </h2>
          <div class="text-gray-300">
            <strong class="text-2xl accent bright">Funding</strong><br>
            <strong>The development of the system is an international endeavor.</strong>
            <br>
            Financial support is sourced from:
            <ul class="text-lg text-gray-300 list-disc ml-4 mt-2 space-y-2">
              <li><strong>Nation States:</strong><br>
                Participating governments, initially superpowers, augmented by AI taxation at the discretion of each
                nation.</li>
              <li><strong>Supranational Organizations:</strong><br>
                Entities such as the UN, or regional alliances
                that prioritize cross-border coordination and governance.</li>
              <li><strong>AI Industry:</strong><br>
                Top industry leaders across frontier labs, infrastructure and hardware.</li>
            </ul>
            <br>
            <strong class="text-2xl accent bright">Blockchain Economy</strong><br>
            Computational system activity is metered using gas tokens:
            <ul class="text-lg text-gray-300 list-disc ml-4 mt-2 space-y-2">
              <li><strong>Closed-Loop Resource Accounting</strong><br>
                Gas tokens are not a tradable cryptocurrency (DeFi) rather internal accounting units generated and
                managed by the system itself.
                They function as a closed-loop "resource rationing" mechanism tied exclusively to system operation.
              </li>
              <li><strong>Algorithmically Defined Pricing</strong><br>
                Gas costs are determined through algorithmic rules that can be updated via top tier policy voters. This
                ensures fairness, prevents volatility, and maintains economic predictability.
              </li>
            </ul>
            <br>
            Gas tokens measure resource use for:
            <ul class="text-lg text-gray-300 list-disc ml-4 mt-2 space-y-2">
              <li><strong>Blockchain Operations:</strong>
                <ul class="list-disc ml-4">
                  <li>Consensus mechanism transactions</li>
                  <li>System state updates</li>
                </ul>
              </li>
              <li><strong>Infrastructure Overheads</strong>
                <ul class="list-disc ml-4">
                  <li>Operational costs of safety mechanisms</li>
                  <li>Hardware inspection costs</li>
                  <li>Shared treasury funding</li>
                </ul>
              </li>
            </ul>
            <br>

            Tokens are purchased for model registration, safety module development, personnel registration, hardware
            registration, and nations to maintain the system token pool.
            <br>
            <br>
            <p><strong>Model training and inference datacenter expenses</strong> (compute, electricity, cloud hosting,
              bandwidth)
              are not
              covered by gas tokens.
              Those are settled externally between datacenters and model registrants. This separation avoids
              over-delegating control to the framework, serving as an off-system security layer.</p>
            <br>

            <p>
              <strong class="text-xl accent bright">System Token Pool for Upkeep</strong><br>
              A system token pool is maintained multilaterally to cover operational costs, such as voting, safety module
              development, and certification renewals.
            </p>
            <br>

            <p>
              <strong class="text-xl accent bright">Shared Treasury for Incentivized Safety</strong><br>
              To incentivize on-system safety research, a shared treasury is funded per-transaction (Gas tax) after an
              initial supply.
              For separation of concerns, an established cryptocurrency is used instead of the system's Gas tokens.
              <br><br>
              During the multi-round safety selection voting, which determines the specific safety modules used, the
              treasury
              awards winning modules for each round. Awards bounties are granted for the discovery of bugs and flaws in
              safety modules or the underlying system itself. This mechanism supports existing AI safety organizations,
              compensating them for release of intellectual property.
            </p>
          </div>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <!-- About Section -->
      <section id="mechanisms" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">
            About
          </h2>
          <p>
            The <i>AI Safety FrameworkZero Organization</i> seeks only to accomplish it's mission: <br>
            <strong>To bring into being standardized safety-first
              global AI development that is mutually advantageous.</strong>
          </p>
          <br>
          <div class="block md:flex">
            <div class="basis-2/5 content-center">
              <img src="landing.jpg" alt="FrameworkZero - International Hybrid-Technical AI Governance DAO"
                class="bg-center mix-blend-screen" />
            </div>
            <div class="content-center basis-3/5 pr-24">
              It's ambition is to enact global coordination, inspire the international will and actualize an AI safety
              standard to prevent catastrophic AI risk.
              It is not a for-profit project.
            </div>
          </div>
          <br>
          <strong class="text-lg">This framework builds off existing research in AI governance.</strong><br>
          Drawing heavily on The Oxford Martin AI Governance Initiative's research, specifically <a class="exlink"
            href="https://aigi.ox.ac.uk/publications/verification-for-international-ai-governance/"
            target="_blank">Harack, 2025</a>, <br>
          as well as
          <a class="exlink" href="https://arxiv.org/pdf/2506.23706" target="_blank">Schnabl, 2025</a>,
          <a class="exlink" href="https://arxiv.org/pdf/2501.08970" target="_blank">Shumailov, 2025</a>,
          <a class="exlink"
            href="https://intelligence.org/wp-content/uploads/2024/11/Mechanisms-to-Verify-International-Agreements-About-AI-Development-27-Nov-24.pdf"
            target="_blank">Scher, 2024</a>,
          <a class="exlink" href="https://arxiv.org/html/2405.06624v3" target="_blank">Dalrymple, 2024</a>,
          and the work of many others.
          <br>
          Inspired by organizations like LawZero & ControlAI, it is therefore <a class="exlink"
            href="https://www.narrowpath.co/" target="_blank">Narrow Path</a> compatible.
          <br>
          <br>
          <strong class="text-xl">Get in touch via the <a href="https://x.com/i/communities/1964271923119989164"
              target="_blank" class="exlink">𝕏 community</a></strong>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <section id="limitations" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">Limitations</h2>
          <ul class="text-lg text-gray-300 list-disc ml-4 space-y-4">
            <li><strong>Time</strong>, this solution assume longer timelines to reaching <span class="tp"
                data-tp="agi">AGI</span>.</li>
            <li><strong>Political will</strong> and recognition of the imminent need for collaborative international AI
              governance. <br>
              Nations therefore must become aware of critical AI risk and realize an AI weapons race benefits none.</li>
            <li><strong>Not designed for <span class="tp" data-tp="agi">AGI</span> or superintelligence;</strong>
              focuses on pre-AGI preventative red
              lines, as <strong>it is not a solution to AI alignment.</strong><br>
              Enduring governance of AGI or superintelligence is a fundamental problem in AI safety research.
              Leading AI scientists hold that it requires superalignment, a concept currently unsolved and possibly
              unsolvable given the timeline set forth by AI race.
            </li>
            <li><strong>Relies on specialized hardware development and cryptographic security.</strong><br>
              There is strong and growing research for the design of specialized hardware with some real-world
              implementation,
              however further development is needed.
              It is the ambition of this framework to enact global coordination and the will to accomplish this.
              <br><br>
              As frontier models grow in capability, so does their
              threat to cryptographic security, escalating the need of critical supervision and ongoing
              reinforcement. This system's many security layers reduce threat impact though it remains an active
              concern.
            </li>
            <li><strong>Limited insight into private model architectures.</strong><br>
              Model <span class="tp" data-tp="template">templates</span>,
              <span class="tp" data-tp="guardrail">guardrails</span> and
              <span class="tp" data-tp="audit">audits</span> reduce, yet do not
              eliminate, risks from dangerous model architectures.</strong>
            </li>
            <li><strong>Model lifecycle monitoring</strong> covers a model's phases and continual learning checkpoints.
              <br>
              Continual learning models include all paradigms of online/streaming continual/lifelong‑learning systems
              (e.g. real‑time/frequent reinforcement‑learning) or with any form of meta-learning (short of RSI).
              For such models, this framework has limited control, and
              likely inadequate for more advanced models of this category. Preventative measures and mitigation
              are achieved by model architecture templating, guardrails, audits, and voting revision calls.
            </li>
            <li><strong>Off-system training remains possible,</strong> though heavily hindered by global mandates on
              datacenters. Frontier AI advancements could produce model architectures that require minimal compute for
              training, running on standard hardware.</li>
            <li><strong>Does not fully govern model inference.</strong><br>
              While inference at participating AI-compute datacenters can only happen with approved, certified models,
              governance of inference input/output is beyond the scope of the system.
              On-chain system safeguards for inference input/output could be added to this framework,
              though the framework itself lacks ability to enforce it.
              <br><br>
              Inference requires fast and low-latency processing at scale, which conflicts with the slower and
              resource-intensive nature of blockchain.
              Neither does inference necessitate AI-compute hardware, with current frontier open-weight models already
              runnable on edge devices - a trend likely to hold as AI models and hardware continues to advance.
            </li>
          </ul>
        </div>
      </section>

      <hr class="hr-gradient my-16">

      <!-- Detailed Mechanisms Section -->
      <section id="mechanisms" class="section-hidden">
        <div class="content-card p-6 md:p-10 rounded-md text-lg">
          <h2 class="text-3xl md:text-4xl font-bold mb-4 text-gradient bright menu-nav-target-title">
            Appendix
          </h2>
          <ul class="list-inside space-y-12 text-gray-300">

            <li id="apdx-tee"><strong class="text-xl accent bright">Trusted Execution Environments (TEEs):</strong><br>
              TEEs provide secure and privacy-preserving code execution in a sealed virtual environment.
              They allow the system to securely deploy model containers and run workloads. They produce reports without
              disclosing intellectual property details. They are utilized when running guardrails and audits on model
              containers and
              post-workload output (attestable audits).
              <br>
              <a class="exlink" href="https://arxiv.org/pdf/2506.23706" target="_blank">Schnabl, 2025</a>
            </li>

            <li id="apdx-tcme"><strong class="text-xl accent bright">Trusted Capable Model Environments
                (TCMEs):</strong><br>
              TCMEs allow a stateless AI model to be instructed to privately verify model container and workload code to
              detect
              banned patterns and red-line violations (e.g. RSI, unapproved architecture, or dangerous algorithms).
              These Trusted Capable Models are 'trusted' in that they act as a neutral mediating party with agreed
              objective (set of instructions) and output.
              They operate within a sealed environment, outputting a verification report without disclosing
              intellectual
              property details.
              <br>
              <br>
              They are not infallible as they are limited by the confidence and capabilities of the
              underlying
              models (Capable Models).
              Despite this, they are an invaluable tool to explore closed-source code bases and
              private post-Workload output.
              <br>
              <a class="exlink" href="https://arxiv.org/pdf/2501.08970" target="_blank">Shumailov, 2025</a>
            </li>

            <li id="apdx-gsai"><strong class="text-xl accent bright">Guaranteed Safe AI (GSAI):</strong><br>
              A framework that provides quantifiable guarantees of AI safety, encapsulated as guardrail checks.
              <br><br>
              "Approaches that follow the GSAI framework aim to provide the level of quantitative safety guarantees we've
              come to expect from other engineered systems. This is achieved with three core components: an auditable,
              separable world model, a way to describe what portions of the state space are 'safe' and 'unsafe', and a
              verifier (which provides an auditable proof certificate that the output satisfy the safety specification
              in the world model)." - <a class="exlink" href="https://www.gsais.org/#gsai" target="_blank">from GSAIS.org</a>
            </li>

            <li id="apdx-rsi"><strong class="text-xl accent bright">Recursive Self-Improvement (RSI)
                Mitigation</strong><br>
              RSI is mitigated through (A) model registration
              (slowing releases via voting), (B) model architecture templates (C) pre-workload guardrails (via TCMEs)
              and (D) compute budgets.
              It is important to note that total elimination of these concerns is not feasible given the development of
              novel architectures and algorithmic improvements.
            </li>

            <li id="apdx-phase"><strong class="text-xl accent bright">Phase Detection:</strong><br>
              Reliably detecting a model's Phase requires adjustments, and may even become infeasible as architectures
              and hardware advance.
              <div class="overflow-x-auto">
                <table class="w-full text-left table-auto overflow-hidden rounded-md">
                  <thead class="primary-bg primary-b">
                    <tr>
                      <th class="p-2 md:p-4"></th>
                      <th class="p-2 md:p-4">Pre-train</th>
                      <th class="p-2 md:p-4">Post-train</th>
                      <th class="p-2 md:p-4">Inference</th>
                    </tr>
                  </thead>
                  <tbody class="divide-y divide-gray-700 primary-b">
                    <tr>
                      <td class="p-2 md:p-4">Data Used</td>
                      <td class="p-2 md:p-4">Massive, raw, unlabeled corpus (web, books, etc.)</td>
                      <td class="p-2 md:p-4">Small, curated/labeled & task/instructional data</td>
                      <td class="p-2 md:p-4">User/user scenario input</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Compute</td>
                      <td class="p-2 md:p-4">Very high (multi-week/month cluster jobs, huge GPU fleets)</td>
                      <td class="p-2 md:p-4">Much lower (hours to days, single/few GPUs)</td>
                      <td class="p-2 md:p-4">Very low (real-time or near real-time)</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Precision</td>
                      <td class="p-2 md:p-4">FP16/BF16 (float, mixed precision for gradients)</td>
                      <td class="p-2 md:p-4">Often FP16/BF16, sometimes lower (efficient tuning)</td>
                      <td class="p-2 md:p-4">FP8/INT8 (quantized for efficiency)</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Memory Usage</td>
                      <td class="p-2 md:p-4">Extremely high (80–141GB+ per GPU, multi-node)</td>
                      <td class="p-2 md:p-4">Moderate/high (but often single node/fewer GPUs)</td>
                      <td class="p-2 md:p-4">Low (10–20GB per GPU typical)</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Batch Size</td>
                      <td class="p-2 md:p-4">Large (512–4096+ for throughput)</td>
                      <td class="p-2 md:p-4">Smaller (8–128, stability/convergence focus)</td>
                      <td class="p-2 md:p-4">Small (1–32 for low latency)</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Latency focus</td>
                      <td class="p-2 md:p-4">Prioritizes throughput, not latency</td>
                      <td class="p-2 md:p-4">Throughput focus, latency not critical</td>
                      <td class="p-2 md:p-4">Low latency (&lt;1s, &lt;100ms per query)</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Energy Use</td>
                      <td class="p-2 md:p-4">Extremely high (100s kWh to MWh total)</td>
                      <td class="p-2 md:p-4">Much lower (1–10% of pre-training consumption)</td>
                      <td class="p-2 md:p-4">Very low (watts per query)</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Accelerators</td>
                      <td class="p-2 md:p-4">Full multi-GPU with NVLink, top-end GPU clusters</td>
                      <td class="p-2 md:p-4">Single/few GPUs (no or minimal NVLink)</td>
                      <td class="p-2 md:p-4">Any GPU/CPU, efficiency prioritized</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Workload Type</td>
                      <td class="p-2 md:p-4">Forward & backward (backprop), full parameter updates</td>
                      <td class="p-2 md:p-4">Same; may use only a subset (adapters/LoRA/PEFT)</td>
                      <td class="p-2 md:p-4">Forward pass only</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Duration</td>
                      <td class="p-2 md:p-4">Weeks–months (large runs)</td>
                      <td class="p-2 md:p-4">Hours–days (sometimes weeks for large/continual)</td>
                      <td class="p-2 md:p-4">Milliseconds–seconds per query</td>
                    </tr>
                    <tr>
                      <td class="p-2 md:p-4">Output</td>
                      <td class="p-2 md:p-4">Foundational (“base”) model, not user-ready</td>
                      <td class="p-2 md:p-4">Aligned/specialized, user-ready model</td>
                      <td class="p-2 md:p-4">Answers, completions, predictions</td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </li>
          </ul>
          <a class="exlink"
            href="https://intelligence.org/wp-content/uploads/2024/11/Mechanisms-to-Verify-International-Agreements-About-AI-Development-27-Nov-24.pdf"
            target="_blank">Scher, 2024</a>
        </div>
      </section>

    </div>

  </div>

  <script>
    // Versioning
    const versionNumber = "1.05"
    window.addEventListener('load', () => {
      const versionElements = document.getElementsByClassName('version-number')
      Array.from(versionElements).forEach(e => e.textContent = versionNumber)
    })
  </script>

</body>

</html>